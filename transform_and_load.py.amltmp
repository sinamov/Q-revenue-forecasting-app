# transform_and_load.py
import os
import io
import pandas as pd
from sqlalchemy import create_engine, text # ✅ --- 1. IMPORT THE 'text' FUNCTION ---
from azure.storage.filedatalake import DataLakeServiceClient
from dotenv import load_dotenv
import logging

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# --- Configuration ---
SYNAPSE_CONN_STRING = os.getenv("SYNAPSE_CONN_STRING")
ADLS_CONNECTION_STRING = os.getenv("ADLS_CONNECTION_STRING")
CONTAINER_NAME = "processed"
OUTPUT_FILE_NAME = "fct_financials_quarterly.parquet"

def run_hybrid_transformation():
    """
    Connects to Synapse to read from clean SQL views, joins them in pandas,
    and uploads the final fact table to the data lake.
    """
    if not SYNAPSE_CONN_STRING or not ADLS_CONNECTION_STRING:
        raise ValueError("SYNAPSE_CONN_STRING and ADLS_CONNECTION_STRING must be set.")

    engine = create_engine(SYNAPSE_CONN_STRING, connect_args={'timeout': 60})
    
    with engine.connect() as connection:
        logging.info("Reading cleaned financials data from Synapse view 'v_clean_financials'...")
        # ✅ --- 2. WRAP THE SQL STRING IN THE text() FUNCTION ---
        df_financials = pd.read_sql_query(text("SELECT * FROM v_clean_financials"), connection)
        
        logging.info("Reading cleaned macro data from Synapse view 'v_clean_fred_macro'...")
        # ✅ --- 3. WRAP THIS SQL STRING AS WELL ---
        df_macro = pd.read_sql_query(text("SELECT * FROM v_clean_fred_macro"), connection)

    # --- Perform the Join in Pandas ---
    df_financials['prediction_quarter'] = pd.to_datetime(df_financials['prediction_quarter'])
    df_macro['prediction_quarter'] = pd.to_datetime(df_macro['prediction_quarter'])

    df_final = pd.merge(df_financials, df_macro, on='prediction_quarter', how='left')
    logging.info(f"Successfully joined data. Final row count: {len(df_final)}")

    # --- Upload the Final 'Fact' Table to the Data Lake ---
    parquet_buffer = io.BytesIO()
    df_final.to_parquet(parquet_buffer, index=False)
    parquet_buffer.seek(0)

    service_client = DataLakeServiceClient.from_connection_string(ADLS_CONNECTION_STRING)
    file_system_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)
    file_client = file_system_client.get_file_client(OUTPUT_FILE_NAME)

    logging.info(f"Uploading {OUTPUT_FILE_NAME} to data lake...")
    file_client.upload_data(parquet_buffer.read(), overwrite=True)
    logging.info("Upload complete. Final data model is ready. ✅")

if __name__ == "__main__":
    run_hybrid_transformation()