# upload_raw_data.py
import os
import glob
import io
from azure.storage.filedatalake import DataLakeServiceClient
from dotenv import load_dotenv
import logging

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# --- Configuration ---
ADLS_CONNECTION_STRING = os.getenv("ADLS_CONNECTION_STRING")
CONTAINER_NAME = "raw" # Uploading to a 'raw' data container

def upload_all_parquet_to_datalake():
    if not ADLS_CONNECTION_STRING:
        raise ValueError("ADLS_CONNECTION_STRING environment variable must be set.")

    try:
        service_client = DataLakeServiceClient.from_connection_string(ADLS_CONNECTION_STRING)
        file_system_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)
        if not file_system_client.exists():
            file_system_client.create_file_system()

        parquet_files = glob.glob('*.parquet')
        if not parquet_files:
            logging.warning("No Parquet files found in the current directory.")
            return

        for file_path in parquet_files:
            file_name = os.path.basename(file_path)
            file_client = file_system_client.get_file_client(file_name)
            
            with open(file_path, "rb") as data:
                logging.info(f"Uploading {file_name} to data lake container '{CONTAINER_NAME}'...")
                file_client.upload_data(data, overwrite=True)
        
        logging.info("All raw Parquet files uploaded successfully.")

    except Exception as e:
        logging.error(f"An error occurred during data lake upload: {e}")

if __name__ == "__main__":
    upload_all_parquet_to_datalake()