# combine_and_load.py
import os
import glob
import io
import pandas as pd
from azure.storage.filedatalake import DataLakeServiceClient
from dotenv import load_dotenv
import logging

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# --- Configuration ---
ADLS_CONNECTION_STRING = os.getenv("ADLS_CONNECTION_STRING")
CONTAINER_NAME = "processed"  # A folder in your data lake to store clean data
FILE_NAME = "stg_financials.parquet" # The output file

def combine_and_upload_to_datalake():
    """
    Finds all local Parquet files, combines them, and uploads the result
    as a single Parquet file to Azure Data Lake Storage.
    """
    if not ADLS_CONNECTION_STRING:
        raise ValueError("ADLS_CONNECTION_STRING environment variable must be set.")

    parquet_files = glob.glob('*_financials.parquet')
    if not parquet_files:
        logging.warning("No Parquet files found. Exiting.")
        return

    logging.info(f"Found files to combine: {parquet_files}")
    df_list = [pd.read_parquet(file) for file in parquet_files]
    final_df = pd.concat(df_list, ignore_index=True)
    logging.info(f"Combined data into a single DataFrame with {len(final_df)} rows.")

    # --- Upload to Azure Data Lake ---
    try:
        # Convert DataFrame to a Parquet file in memory
        parquet_buffer = io.BytesIO()
        final_df.to_parquet(parquet_buffer, index=False)
        parquet_buffer.seek(0)

        # Create a client to interact with the data lake
        service_client = DataLakeServiceClient.from_connection_string(ADLS_CONNECTION_STRING)
        
        # Get a client for the container (folder)
        file_system_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)
        # Create the container if it doesn't exist
        if not file_system_client.exists():
            file_system_client.create_file_system()

        # Get a client for the file
        file_client = file_system_client.get_file_client(FILE_NAME)

        logging.info(f"Uploading {FILE_NAME} to data lake container '{CONTAINER_NAME}'...")
        file_client.upload_data(parquet_buffer.read(), overwrite=True)
        
        logging.info("Upload complete. âœ…")

    except Exception as e:
        logging.error(f"An error occurred during data lake upload: {e}")

if __name__ == "__main__":
    combine_and_upload_to_datalake()