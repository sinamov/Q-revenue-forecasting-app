# ingest_data.py
import os
import time
import pandas as pd
from alpha_vantage.fundamentaldata import FundamentalData
from dotenv import load_dotenv
import logging
import re

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# --- Configuration ---
TICKERS_TO_RUN = ["GOOGL"] 
ALPHA_VANTAGE_KEY = os.getenv("ALPHA_VANTAGE_API_KEY")

def snake_case_columns(df):
    """Converts DataFrame columns from camelCase to snake_case."""
    cols = df.columns
    new_cols = [re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower() for col in cols]
    df.columns = new_cols
    return df

def fetch_and_cache_data():
    """
    Fetches financial data from Alpha Vantage and saves each ticker to its own
    local Parquet file, overwriting any previous data.
    """
    if not ALPHA_VANTAGE_KEY:
        raise ValueError("ALPHA_VANTAGE_API_KEY environment variable must be set.")

    fd = FundamentalData(key=ALPHA_VANTAGE_KEY, output_format='pandas')

    for ticker_symbol in TICKERS_TO_RUN:
        cache_file_path = f"{ticker_symbol}_financials.parquet"
        
        try:
            logging.info(f"Fetching data for {ticker_symbol} from Alpha Vantage...")
            income_statement = fd.get_income_statement_quarterly(symbol=ticker_symbol)[0]
            balance_sheet = fd.get_balance_sheet_quarterly(symbol=ticker_symbol)[0]
            cash_flow = fd.get_cash_flow_quarterly(symbol=ticker_symbol)[0]
            
            if income_statement.empty or balance_sheet.empty or cash_flow.empty:
                logging.warning(f"API returned empty data for {ticker_symbol}. Skipping.")
                continue

            income_statement.set_index('fiscalDateEnding', inplace=True)
            balance_sheet.set_index('fiscalDateEnding', inplace=True)
            cash_flow.set_index('fiscalDateEnding', inplace=True)
            
            df = pd.concat([income_statement, balance_sheet, cash_flow], axis=1)
            df = df.loc[:, ~df.columns.duplicated(keep='first')] # Handle duplicate columns
            df.reset_index(inplace=True)

            # Clean up data and column names
            df.replace('None', 0, inplace=True)
            df = snake_case_columns(df)
            df.rename(columns={'fiscal_date_ending': 'report_date'}, inplace=True)

            df['ticker'] = ticker_symbol
            df['loaded_at'] = pd.Timestamp.now(tz='UTC')

            # Convert all numeric columns
            for col in df.columns:
                if col not in ['report_date', 'ticker', 'loaded_at']:
                     df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

            # Overwrite the old file
            df.to_parquet(cache_file_path)
            logging.info(f"Successfully saved {ticker_symbol} data to {cache_file_path}.")

            logging.info("Waiting 15 seconds to respect API rate limit...")
            time.sleep(15)

        except Exception as e:
            logging.error(f"Failed to process data for {ticker_symbol}: {e}")
            break

if __name__ == "__main__":
    fetch_and_cache_data()