# ingest_fred_data.py
import os
import io
import pandas as pd
from fredapi import Fred
from azure.storage.filedatalake import DataLakeServiceClient
from dotenv import load_dotenv
import logging

# --- Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# --- Configuration ---
FRED_API_KEY = os.getenv("FRED_API_KEY")
ADLS_CONNECTION_STRING = os.getenv("ADLS_CONNECTION_STRING")
CONTAINER_NAME = "processed"
FILE_NAME = "stg_fred_macro_quarterly.parquet"

# FRED series IDs for key quarterly indicators
SERIES_IDS = {
    'GDP': 'GDP', # Gross Domestic Product
    'CPI': 'CPIAUCSL', # Consumer Price Index
    'UNEMPLOYMENT': 'UNRATE' # Unemployment Rate
}

def fetch_and_upload_fred_data():
    """
    Fetches macroeconomic data from FRED, processes it to quarterly averages,
    and uploads it as a Parquet file to Azure Data Lake.
    """
    if not FRED_API_KEY or not ADLS_CONNECTION_STRING:
        raise ValueError("FRED_API_KEY and ADLS_CONNECTION_STRING must be set.")

    try:
        fred = Fred(api_key=FRED_API_KEY)
        
        all_series = []
        for name, series_id in SERIES_IDS.items():
            logging.info(f"Fetching {name} ({series_id})...")
            # Fetch the data (might be daily or monthly)
            s = fred.get_series(series_id)
            # Resample to get the average value per quarter, then forward-fill any gaps
            s = s.resample('QS').mean().ffill()
            s.name = name.lower()
            all_series.append(s)

        # Combine all series into a single DataFrame
        df = pd.concat(all_series, axis=1).reset_index()
        df.rename(columns={'index': 'report_date'}, inplace=True)
        df['report_date'] = pd.to_datetime(df['report_date']).dt.date
        
        logging.info(f"Successfully processed {len(df)} quarters of macroeconomic data.")

        # --- Upload to Azure Data Lake ---
        parquet_buffer = io.BytesIO()
        df.to_parquet(parquet_buffer, index=False)
        parquet_buffer.seek(0)
        
        service_client = DataLakeServiceClient.from_connection_string(ADLS_CONNECTION_STRING)
        file_system_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)
        file_client = file_system_client.get_file_client(FILE_NAME)

        logging.info(f"Uploading {FILE_NAME} to data lake container '{CONTAINER_NAME}'...")
        file_client.upload_data(parquet_buffer.read(), overwrite=True)
        logging.info("Upload complete.")

    except Exception as e:
        logging.error(f"An error occurred: {e}")

if __name__ == "__main__":
    fetch_and_upload_fred_data()